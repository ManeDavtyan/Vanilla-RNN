{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed78d38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class VanillaRNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size, optimizer=\"GD\"):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        #initializing parameters\n",
    "        self.W_xh = np.random.randn(self.hidden_size, self.input_size)\n",
    "        self.W_hh = np.random.randn(self.hidden_size, self.hidden_size)\n",
    "        self.W_hy = np.random.randn(self.output_size, self.hidden_size)\n",
    "        self.b_h = np.zeros((self.hidden_size, 1))\n",
    "        self.b_o = np.zeros((self.output_size, 1))\n",
    "        self.hidden_state = np.zeros((self.hidden_size, 1))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        h = np.zeros((self.hidden_size, 1))\n",
    "        y = np.zeros((self.input_size, self.output_size))\n",
    "\n",
    "        for t in range(self.input_size):\n",
    "            x = inputs[t].reshape((self.input_size, 1))\n",
    "\n",
    "            h = self.tanh(np.dot(self.W_hh, h) + np.dot(self.W_xh, x) + self.b_h) \n",
    "            y[t] = (np.dot(self.W_hy, h) + self.b_o).reshape((self.output_size,))\n",
    "            \n",
    "        return y\n",
    "\n",
    "\n",
    "    def backward(self, inputs, targets):\n",
    "        dW_xh = np.zeros_like(self.W_xh)\n",
    "        dW_hh = np.zeros_like(self.W_hh)\n",
    "        dW_hy = np.zeros_like(self.W_hy)\n",
    "        db_o = np.zeros_like(self.b_o)\n",
    "        db_h = np.zeros_like(self.b_h)\n",
    "        dhidden_next = np.zeros_like(self.hidden_state)\n",
    "        dy = np.zeros_like(targets)\n",
    "        dh = np.zeros_like(self.hidden_state)\n",
    "\n",
    "        for t in reversed(range(self.input_size)):\n",
    "            dy[t] = self.forward(inputs) - targets\n",
    "            dW_hy += np.dot(dy[t], h.T)\n",
    "            db_o += dy[t]\n",
    "            dh = np.dot(self.W_hy.T, dy[t]) + dhidden_next\n",
    "            dh_raw = self.dtanh(h) * dh\n",
    "            db_h += dh_raw\n",
    "            dW_xh += np.dot(dh_raw, inputs[t].T)\n",
    "            dW_hh += np.dot(dh_raw, h.T)\n",
    "            dhidden_next = np.dot(self.W_hh.T, dh_raw)\n",
    "\n",
    "        return dW_xh, dW_hh, dW_hy, db_o, db_h\n",
    "\n",
    "    \n",
    "    def update_parameters(self, dW_xh, dW_hh, dW_hy, db_o, db_h, learning_rate=0.01):\n",
    "        if self.optimizer == \"GD\":\n",
    "            self.W_xh -= learning_rate * dW_xh\n",
    "            self.W_hh -= learning_rate * dW_hh\n",
    "            self.W_hy -= learning_rate * dW_hy\n",
    "            self.b_h -= learning_rate * db_h\n",
    "            self.b_o -= learning_rate * db_o\n",
    "        elif self.optimizer == \"AdaGrad\":\n",
    "            epsilon = 1e-8\n",
    "            self.W_ih += -learning_rate * dW_ih / (np.sqrt(np.square(dW_ih) + epsilon))\n",
    "            self.W_hh += -learning_rate * dW_hh / (np.sqrt(np.square(dW_hh) + epsilon))\n",
    "            self.W_ho += -learning_rate * dW_ho / (np.sqrt(np.square(dW_ho) + epsilon))\n",
    "            self.b_h += -learning_rate * db_h / (np.sqrt(np.square(db_h) + epsilon))\n",
    "            self.b_o += -learning_rate * db_o / (np.sqrt(np.square(db_o) + epsilon))\n",
    "            \n",
    "        elif self.optimizer == \"RMSprop\":\n",
    "            decay_rate = 0.9\n",
    "            self.squared_gradient_W_ih = decay_rate * self.squared_gradient_W_ih + (1 - decay_rate) * np.square(dW_ih)\n",
    "            self.W_ih -= learning_rate * dW_ih / (np.sqrt(self.squared_gradient_W_ih) + epsilon)\n",
    "            \n",
    "            self.squared_gradient_W_hh = decay_rate * self.squared_gradient_W_hh + (1 - decay_rate) * np.square(dW_hh)\n",
    "            self.W_hh -= learning_rate * dW_hh / (np.sqrt(self.squared_gradient_W_hh) + epsilon)\n",
    "            \n",
    "            self.squared_gradient_W_ho = decay_rate * self.squared_gradient_W_ho + (1 - decay_rate) * np.square(dW_ho)\n",
    "            self.W_ho -= learning_rate * dW_ho / (np.sqrt(self.squared_gradient_W_ho) + epsilon)\n",
    "\n",
    "            self.squared_gradient_b_h = decay_rate * self.squared_gradient_b_h + (1 - decay_rate) * np.square(db_h)\n",
    "            self.b_h -= learning_rate * db_h / (np.sqrt(self.squared_gradient_b_h) + epsilon)\n",
    "\n",
    "            self.squared_gradient_b_o = decay_rate * self.squared_gradient_b_o + (1 - decay_rate) * np.square(db_o)\n",
    "            self.b_o -= learning_rate * db_o / (np.sqrt(self.squared_gradient_b_o) + epsilon)\n",
    "    @staticmethod        \n",
    "    def tanh(x):\n",
    "        return (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
    "    @staticmethod \n",
    "    def dtanh(x):\n",
    "        return 1 - self.tanh(x)**2\n",
    "    \n",
    "    def loss(self, predicted, target):\n",
    "        #categorical cross entropy \n",
    "        predicted = predicted.T  \n",
    "        loss = -np.sum(target * np.log(predicted + 1e-7)) #1e-7 to avoid zero division error\n",
    "        return loss\n",
    "\n",
    "    def fit(self, inputs, targets, learning_rate=0.01, epochs=100, batch_size=1):\n",
    "        for epoch in range(epochs):\n",
    "            Loss = 0 #total loss\n",
    "            for i in range(0, len(inputs), batch_size):\n",
    "                batch_inputs = inputs[i:i+batch_size]\n",
    "                batch_targets = targets[i:i+batch_size]\n",
    "\n",
    "                y = self.forward(batch_inputs)\n",
    "                loss = self.loss(y, batch_targets)\n",
    "                gradients = self.backward(batch_inputs, batch_targets)\n",
    "                self.update_parameters(*gradients, learning_rate)\n",
    "                Loss += loss\n",
    "\n",
    "            average_loss = total_loss / (len(inputs) / batch_size)\n",
    "\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch: {epoch+1}, Loss: {average_loss}\")\n",
    "\n",
    "        \n",
    "\n",
    "    def predict(self, inputs, targets):\n",
    "        y = self.forward(inputs)\n",
    "        loss = self.loss(y, targets)\n",
    "        print(f\"Test Loss: {loss}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
